## CEO/CTO/Docker intro

Docker 1.0

DockerHub w/ private hosted repos


## Rackspace CTO keynote

Trivial and pervasice containers

Grassroots effort led to docker adoption at Rackspace

  * Three weekends to port mailgun to Docker containers

Rackspace wants to use Docker to ease orchestration and updates


Shipper - fabric for Docker

Project Solum - OpenStack Heroku Style

  * https://wiki.openstack.org/wiki/Solum

Native cloud support for Docker


## RedHat EVP/CTO Brian Stevens

Technical difficulties for first 5 minutes.

Presentation with reveal.js!

13 years at RedHat

100x advantage to replace previous standard

Infrastructure plumbers -> building houses (Project Atomic)

The journey to containers for RedHat started with SELinux and an NSA contract.
Once Google released cgroups, RedHat immediately recognized  the usefulness.

### Project Atomic (http://www.projectatomic.io/)

Targeted for container based deployments, image based updates, rollback

Facilities for managing containers (GearD)

  * Bridging systemd to containers/Docker

Sounds a lot like CoreOS but run by RedHat

Source -> container + Container wiring (networking)

Dynamic DNS is hard

Cockpit GUI for managing containers

  * Atomic installation is 321 packages versus 1419 for Fedora
    * Though uh, he does have a whole GUI installed on Fedora
  * WebUI (Cockpit) uses journald to show live logs
  * Resource monitoring by container
  * Dynamic resource limits
  * Marketing point is '200+ kernel committers'


Cynical aside: node + mongo + load balancer not for reliability,
        but because they have to?

Demo using Apache JMeter + automatically discovered services that
are added to haproxy config.

Scaling with Mesosphere + GearD

  * http://openshift.github.com/geard

  * http://projectatomic.io

  * http://mesosphere.io/geard

## Spotify:Helios

## Facebook:Tupperware

Datacenter with clusters
Cluster with racks
Rack with machines

Scheduler + ServerDB + Agent on machines

Scope of scheduler is cluster of machines

Scheduler decides where on cluster to run machines

Agent ensures tasks are running on individual machines

Services are defined by spec files (config.tw)

  * What binary?
  * Command line args
  * Dependencies

twdeploy submits job to scheduler, which talks to ServerDB
and determines hosts to run the service.  Job submitted to
server side agent, which uses the Bittorrent based
distribution system to pull the binary down

### Failover:

  * 17 machines / hour failure rate
  * Datacenter/Cluster decom
  * Scheduler heartbeats agents
  * On heartbeat failure or machine failure notification, job is
    rescheduled automatically using the same scheduling algorithm
  * Also handles preemptive task migration for maintenance, etc

### Scheduler constraints

  * "Must be production host"
  * "Requires entire machine"
  * "Must be colocated with service X"
  * "At most N instances on same machine"

### Tupperware agent

  * Composed of TaskManager, API, ResourceManager, PackageManager, Heartbeat
  * Launches an AgentHelper that manages individual tasks, heartbeats with agent
  * AgentHelper allows Agent to be upgraded without killing all jobs
  * Sounds like distribution is via RPM

### Logging

  * Persistent logs
  * Instantaneous rotation with transparent compression

### Container implementations

Initially, facebook used chroots to contain their processes.  However,
there was no isolation and chroots are considered insecure.  Root in chroot
is root on the whole machine and proc is not namespaced.

Switched to Linux containers using separate process and file namespaces

  * Required resources are mounted directly into the container using bind
    mounts.  By default this does not happen.

SSH daemon running within each container, which allows them to manage and
inspect those hosts without having to log into the base host.

### Configuration

Job configuration defines profile, packages, scheduling, resource limits, etc.
Dialect of Python with library support (import "git.tw")

### Resource Limits

Uses cgroup notification API to handle memory limits, allowing for adaptive
limits.

Considers historical usage of your task when scheduling and killing tasks

### Hurdles chroots->containers

Monitoring missed process namespaced services

Most problems were social, not technical, as there was no immediate benefit.

### Service discovery

When the scheduler puts services on arbitrary hosts, it also updates the 
ServiceRegistry to list instances of the service.  All clients connect to
the ServiceRegistry to locate services, allowing them to handle the failure
of individual instances.

### Monitoring and Alerting

  * Collects statistics
    * CPU, Memory, Disk usage
    * Number of clients
    * Number of exceptions
    * Response time
  * Custom health checks based on stats

### In defense of NIH

Docker / CoreOS did not exist at the time
VMs have a performance penalty and hyeprvisors make debugging harder

### Lessons learned

  * Releases are scary
    * Release often
    * Dry runs
    * Canaries
    * Manage dependencies

  * Sane defaults are necessary

  * Hard to understand *why* tupperware did something and what went wrong
    * How do I fix it?

  * Automated deployment means less work to deploy for engineers

None of this is open source at this time.


## Docker @ RelateIQ

Project planning never involved operations engineers until
the product was ready to be pushed to production.

Turnaround time for dev->prod was weeks-months

Can't test everything.

Hackday after discovering Docker, put Mongo, Cassandra, and
other things into containers easily.

Created an entire 'devenv' around docker

Transparent upgrades / zero downtime using hipache.

Ubiquitous deployment

## Cluster Management and Containerizaton
Benjamin Hindman, Twitter (One of the creators of Mesos)

Original title was "High-Speed Shipping Lanes:
How Containers are Revolutionizing ## Distributed Computing At Scale"

### cluster management includes server/IT automation

  1. configuration/package management
  1. deployment
  1. naming
  1. monitoring (not part of talk)

#### configuration/package management

What/how do things get installed?

  * ssh $host <cmd>

#### deployment

What should run where?

How should it be started/stopped?

  * scp/git/other
  * monit/script/etc

#### naming

How should apps find each other?

  * hosts files, etc

### scale requires automation

Twitter (~2010) used

  * puppet for configuration/package management and
  * capistrano for deployment

This didn't handle failures well.

Example: "Had to reboot box!  Were there any prod services?"

_Fault domains_

This didn't handle maintenance well (planned failures).

  * Upgrading software
  * Replacing hardware

This didn't lead to great utilization because machines are not shared


### adopting cluster management practices from academia

Different scales (10s versus 1000s)
Different prod software (compute jobs versus web services)

Cluster manager manages the resources in a cluster - which jobs should be run
on which computers.

Mesos represents a modern _general purpose_ scheduler

  * service
  * batch
  * storage
  * streaming

Mesos
  * enables elasticity and rebalancing between different schedulers.
  * promotes multi-tenancy on individual machines
  * uses containers

Docker integration introduced in 2014


### cluster management at Twitter today

1. bundle services as jar, tar/gzip
1. upload to HDFS

The above will migrate to Docker

For deployments, Apache Aurora scheduler for Mesos designed for deploying
stateless services.

Naming handled by Apache ZooKeeper and server sets.  Service registers with
Zookeper after it is launched using Mesos.  Other services use ZooKeeper
to find services.
