## CEO/CTO/Docker intro

Docker 1.0

DockerHub w/ private hosted repos


## Rackspace CTO keynote

Trivial and pervasice containers

Grassroots effort led to docker adoption at Rackspace

  * Three weekends to port mailgun to Docker containers

Rackspace wants to use Docker to ease orchestration and updates


Shipper - fabric for Docker

Project Solum - OpenStack Heroku Style

  * https://wiki.openstack.org/wiki/Solum

Native cloud support for Docker


## RedHat EVP/CTO Brian Stevens

Technical difficulties for first 5 minutes.

Presentation with reveal.js!

13 years at RedHat

100x advantage to replace previous standard

Infrastructure plumbers -> building houses (Project Atomic)

The journey to containers for RedHat started with SELinux and an NSA contract.
Once Google released cgroups, RedHat immediately recognized  the usefulness.

### Project Atomic (http://www.projectatomic.io/)

Targeted for container based deployments, image based updates, rollback

Facilities for managing containers (GearD)

  * Bridging systemd to containers/Docker

Sounds a lot like CoreOS but run by RedHat

Source -> container + Container wiring (networking)

Dynamic DNS is hard

Cockpit GUI for managing containers

  * Atomic installation is 321 packages versus 1419 for Fedora
    * Though uh, he does have a whole GUI installed on Fedora
  * WebUI (Cockpit) uses journald to show live logs
  * Resource monitoring by container
  * Dynamic resource limits
  * Marketing point is '200+ kernel committers'


Cynical aside: node + mongo + load balancer not for reliability,
        but because they have to?

Demo using Apache JMeter + automatically discovered services that
are added to haproxy config.

Scaling with Mesosphere + GearD

  * http://openshift.github.com/geard

  * http://projectatomic.io

  * http://mesosphere.io/geard

## Spotify:Helios

## Facebook:Tupperware

Datacenter with clusters
Cluster with racks
Rack with machines

Scheduler + ServerDB + Agent on machines

Scope of scheduler is cluster of machines

Scheduler decides where on cluster to run machines

Agent ensures tasks are running on individual machines

Services are defined by spec files (config.tw)

  * What binary?
  * Command line args
  * Dependencies

twdeploy submits job to scheduler, which talks to ServerDB
and determines hosts to run the service.  Job submitted to
server side agent, which uses the Bittorrent based
distribution system to pull the binary down

### Failover:

  * 17 machines / hour failure rate
  * Datacenter/Cluster decom
  * Scheduler heartbeats agents
  * On heartbeat failure or machine failure notification, job is
    rescheduled automatically using the same scheduling algorithm
  * Also handles preemptive task migration for maintenance, etc

### Scheduler constraints

  * "Must be production host"
  * "Requires entire machine"
  * "Must be colocated with service X"
  * "At most N instances on same machine"

### Tupperware agent

  * Composed of TaskManager, API, ResourceManager, PackageManager, Heartbeat
  * Launches an AgentHelper that manages individual tasks, heartbeats with agent
  * AgentHelper allows Agent to be upgraded without killing all jobs
  * Sounds like distribution is via RPM

### Logging

  * Persistent logs
  * Instantaneous rotation with transparent compression

### Container implementations

Initially, facebook used chroots to contain their processes.  However,
there was no isolation and chroots are considered insecure.  Root in chroot
is root on the whole machine and proc is not namespaced.

Switched to Linux containers using separate process and file namespaces

  * Required resources are mounted directly into the container using bind
    mounts.  By default this does not happen.

SSH daemon running within each container, which allows them to manage and
inspect those hosts without having to log into the base host.

### Configuration

Job configuration defines profile, packages, scheduling, resource limits, etc.
Dialect of Python with library support (import "git.tw")

### Resource Limits

Uses cgroup notification API to handle memory limits, allowing for adaptive
limits.

Considers historical usage of your task when scheduling and killing tasks

### Hurdles chroots->containers

Monitoring missed process namespaced services

Most problems were social, not technical, as there was no immediate benefit.

### Service discovery

When the scheduler puts services on arbitrary hosts, it also updates the 
ServiceRegistry to list instances of the service.  All clients connect to
the ServiceRegistry to locate services, allowing them to handle the failure
of individual instances.

### Monitoring and Alerting

  * Collects statistics
    * CPU, Memory, Disk usage
    * Number of clients
    * Number of exceptions
    * Response time
  * Custom health checks based on stats

### In defense of NIH

Docker / CoreOS did not exist at the time
VMs have a performance penalty and hyeprvisors make debugging harder

### Lessons learned

  * Releases are scary
    * Release often
    * Dry runs
    * Canaries
    * Manage dependencies

  * Sane defaults are necessary

  * Hard to understand *why* tupperware did something and what went wrong
    * How do I fix it?

  * Automated deployment means less work to deploy for engineers

None of this is open source at this time.
